(window.webpackJsonp=window.webpackJsonp||[]).push([[76],{396:function(n,t,e){"use strict";e.r(t);var v=e(18),_=Object(v.a)({},(function(){var n=this,t=n.$createElement,e=n._self._c||t;return e("ContentSlotsDistributor",{attrs:{"slot-key":n.$parent.slotKey}},[e("p",[n._v("大文件上传\n核心\nBlob.prototype.slice")]),n._v(" "),e("p",[n._v("思路\n前端")]),n._v(" "),e("ol",[e("li",[n._v("设置切片的最大数量，将文件切分成一个个小的切片")]),n._v(" "),e("li",[n._v("同时上传多个小切片")]),n._v(" "),e("li",[n._v("并发上传可能会导致上传到服务器的顺序发生变化，因此需要给每个切片记录顺序")])]),n._v(" "),e("p",[n._v("服务端")]),n._v(" "),e("ol",[e("li",[e("p",[n._v("接收切片")])]),n._v(" "),e("li",[e("p",[n._v("合并切片\n2.1 方法1：前端需要在切片中携带切片最大数量的信息，当服务端接收到这个数量的切片时，自动合并\n2.2 方法2：前端发送一个请求通知服务端进行切片的合并")]),n._v(" "),e("p",[n._v("合并实现\nnodejs：使用nodejs的读写流(readStream/writeStream), 将所有切片的流传输到最终的文件流里")])])]),n._v(" "),e("p",[n._v("实现过程\nconst SIZE = 10 * 1024 * 1024; // 切片大小")]),n._v(" "),e("p",[n._v("断点续传\n原理：前端/服务端需要记住已上传的切片，这样下次上传就可以跳过之前已上传的部分\n方案1：前端使用localStorStorge记录已上传的切片Hash   （缺点：换了浏览器就失去了记忆效果）\n方案2：服务端保存已上传文件的hash，前端每次上传前向服务端获取已上传的切片")]),n._v(" "),e("p",[n._v("hash生成方式：\n方案1：文件名+切片下标 （缺点：文件名一但修改就失去了效果）\n方案2：根据文件内容生成hash")]),n._v(" "),e("p",[n._v("spark-md5：可以根据文件内容计算出文件的hash值\n读取文件内容计算 hash 是非常耗费时间的，并且会引起 UI 的阻塞，导致页面假死状态，所以我们使用 web-worker 在 worker 线程计算 hash")])])}),[],!1,null,null,null);t.default=_.exports}}]);